# -*- coding: utf-8 -*-
"""DLProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jmkni9Q2qKhkIn0vMRlHzMrOcF0bu-mr

# **Deep Learning for Newsgroup Classification: A Bidirectional LSTM Approach with Baseline Comparisons**

## Project Overview

In this project, we address the challenge of classifying news articles by leveraging deep learning techniques alongside traditional baseline models. We focus on a Bidirectional LSTM network to capture contextual information from the text, which is crucial for understanding the nuances of language in the 20 Newsgroups dataset. In addition, we have implemented baseline models using both supervised and unsupervised approaches—specifically, a Logistic Regression classifier trained directly on TF‑IDF features (supervised) and a combination of Non-negative Matrix Factorization (NMF) with Logistic Regression (unsupervised)—to provide a comprehensive comparison of performance.

## Dataset Description

The **20 Newsgroups** dataset consists of nearly 20,000 documents categorized into 20 different newsgroups. Each document is labeled according to the newsgroup from which it originated. This dataset is widely used in natural language processing (NLP) research for tasks such as text classification and clustering. Originally sourced from [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/), the dataset is readily accessible via scikit‑learn’s `load_files` function.

## Problem Objective

The primary objective of this project is to classify news articles into their respective categories using a deep learning approach while comparing its performance with traditional baseline methods. To achieve this, we:

- Preprocess the raw text by converting it to lowercase, removing punctuation, and eliminating stopwords.
- Extract numerical features using TF‑IDF.
- Build a deep learning model based on a Bidirectional LSTM architecture to capture contextual dependencies in the text.
- Perform hyperparameter tuning with Keras Tuner to optimize model performance.
- Evaluate the deep learning model and compare it with baseline models:
  - **Supervised Baseline:** Logistic Regression directly on TF‑IDF features.
  - **Unsupervised Baseline:** NMF for latent topic extraction combined with Logistic Regression.
  
These comparisons help to highlight the advantages and limitations of the deep learning approach relative to conventional methods.

## Deep Learning Approach

Our deep learning model employs a Bidirectional LSTM network, which processes input sequences in both forward and backward directions. This architecture enables the model to understand the context surrounding each word, improving its ability to classify text. The model's main components include:

- **Embedding Layer:** Converts word indices into dense vector representations.
- **Bidirectional LSTM Layers:** Capture dependencies from both directions to effectively model the context.
- **Dropout Layers:** Mitigate overfitting by randomly dropping neurons during training.
- **Dense Output Layer:** Uses softmax activation to output probability distributions across the 20 newsgroup categories.

We use Keras Tuner for hyperparameter tuning to determine the optimal values for parameters such as the embedding dimension, number of LSTM units, dropout rates, and learning rate.

## Baseline Models

To evaluate the effectiveness of the deep learning model, we also implemented:

- **Supervised Model:** A Logistic Regression classifier trained directly on TF‑IDF features.
- **Unsupervised Model:** An unsupervised approach where we applied NMF to reduce the dimensionality of the TF‑IDF features, followed by a Logistic Regression classifier trained on the latent features.

These baseline models serve as reference points to assess improvements achieved by the deep learning model.

## Deliverables

1. **Jupyter Notebook Report:** A detailed notebook that documents the problem description, data preprocessing, exploratory data analysis (EDA), deep learning model building and tuning, baseline model implementation, evaluation, and final conclusions.
2. **Video Presentation:** A concise 5–10 minute presentation summarizing the project, including the problem statement, the deep learning and baseline methods used, and key results.
3. **GitHub Repository:** A public repository containing all project code, documentation, and supplementary materials (e.g., a project report or slides).

---

This project demonstrates the application of a deep learning approach using Bidirectional LSTM for text classification, while also comparing its performance against conventional supervised and unsupervised baseline models. The combination of these methods provides a comprehensive analysis of model performance on the 20 Newsgroups dataset.

**Git Link :** https://github.com/spatika1504/DeepLearning
"""

!tar -xvzf 20news-19997.tar.gz

from sklearn.datasets import load_files

# Assuming the folder '20_newsgroups' was extracted to the current directory
data_dir = "20_newsgroups"
newsgroups_data = load_files(data_dir, encoding='latin1', shuffle=True, random_state=42)

print("Number of documents:", len(newsgroups_data.data))
print("Categories:", newsgroups_data.target_names)

"""**Convert Data to DataFrame and Basic EDA**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Create DataFrame from loaded data
df = pd.DataFrame({
    'text': newsgroups_data.data,
    'category': newsgroups_data.target
})
# Map numeric category to category names for readability
target_names = newsgroups_data.target_names
df['category_name'] = df['category'].apply(lambda x: target_names[x])

print("Data shape:", df.shape)
print(df.head())

# Visualize distribution of documents per category
plt.figure(figsize=(12,6))
sns.countplot(x='category_name', data=df, order=pd.value_counts(df['category_name']).index, palette='viridis')
plt.xticks(rotation=45)
plt.title("Number of Documents per Category")
plt.xlabel("Category")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# Compute document length (word count) and visualize distribution
df['word_count'] = df['text'].apply(lambda x: len(x.split()))
plt.figure(figsize=(10,6))
plt.hist(df['word_count'], bins=30, color='skyblue', edgecolor='black')
plt.title("Histogram of Document Word Counts")
plt.xlabel("Word Count")
plt.ylabel("Frequency")
plt.show()

# --- Exploratory Data Analysis (EDA) ---

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import re

# Assuming newsgroups_data has already been loaded from load_files
# Convert the loaded data to a pandas DataFrame
df = pd.DataFrame({
    'text': newsgroups_data.data,
    'category': newsgroups_data.target
})

# Map numeric target to category names for clarity
target_names = newsgroups_data.target_names
df['category_name'] = df['category'].apply(lambda x: target_names[x])

# Display basic information
print("Data shape:", df.shape)
print(df.head())
df.info()

# Check for missing values in the DataFrame
print("Missing values per column:")
print(df.isnull().sum())

# --- Compute Text-Based Numeric Features ---

# Function to compute basic text statistics
def compute_text_stats(text):
    words = text.split()
    word_count = len(words)
    char_count = len(text)
    avg_word_length = np.mean([len(word) for word in words]) if words else 0
    return pd.Series({'word_count': word_count, 'char_count': char_count, 'avg_word_length': avg_word_length})

# Apply the function to compute new features
text_stats = df['text'].apply(compute_text_stats)
df = pd.concat([df, text_stats], axis=1)
print(df.head())

# --- Visualizations ---

# Histogram of word count
plt.figure(figsize=(10,6))
sns.histplot(df['word_count'], bins=30, kde=True, color='skyblue')
plt.title("Histogram of Document Word Counts")
plt.xlabel("Word Count")
plt.ylabel("Frequency")
plt.show()

# Boxplot for word count to check for outliers
plt.figure(figsize=(8,4))
sns.boxplot(x=df['word_count'], color='lightgreen')
plt.title("Boxplot of Document Word Counts")
plt.xlabel("Word Count")
plt.show()

# Histogram of average word length
plt.figure(figsize=(10,6))
sns.histplot(df['avg_word_length'], bins=30, kde=True, color='coral')
plt.title("Histogram of Average Word Length")
plt.xlabel("Average Word Length")
plt.ylabel("Frequency")
plt.show()

# Scatter plot: word count vs average word length
plt.figure(figsize=(8,6))
sns.scatterplot(x='word_count', y='avg_word_length', data=df, hue='category_name', palette='tab20', legend=False)
plt.title("Word Count vs. Average Word Length")
plt.xlabel("Word Count")
plt.ylabel("Average Word Length")
plt.show()

# Correlation heatmap of computed numeric features
features = ['word_count', 'char_count', 'avg_word_length']
corr_matrix = df[features].corr()
plt.figure(figsize=(6,4))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix of Text Statistics")
plt.show()

"""I loaded the 20 Newsgroups data using scikit‑learn’s load_files. The dataset contains about 20,000 documents across 20 categories. My EDA shows a fairly balanced category distribution and documents with word counts mostly ranging between 100 and 1000 words.

Based on the EDA, the dataset contains approximately 20,000 documents across 20 categories.
The text features extracted (word count, character count, and average word length) provide insight into
the structure of the articles. For example, the histogram and boxplot of word counts show that most documents
have between 100 and 1000 words, with some outliers. The correlation heatmap suggests that the word count and
character count are strongly correlated, which is expected since longer texts tend to have more characters.

In addition, the scatter plot between word count and average word length does not show a strong correlation,
indicating that longer articles do not necessarily have longer words on average.

Data cleaning steps that may be necessary include:
  - Removing or correcting outliers if they are due to noise.
  - Checking for and handling missing values (although none were detected in this dataset).

Based on these observations, I plan to preprocess the text by:
  1. Converting all text to lowercase.
  2. Removing punctuation using regular expressions.
  3. Removing stopwords to reduce noise.
After these cleaning steps, I will extract TF-IDF features to represent the documents numerically for further unsupervised analysis.

**Text Preprocessing and TF‑IDF Feature Extraction**
"""

import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('stopwords')

# Cache stopwords once
stop_words_set = set(stopwords.words('english'))

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Remove stopwords using the cached set
    words = text.split()
    filtered = [word for word in words if word not in stop_words_set]
    return " ".join(filtered)

# Apply preprocessing
df['text_preprocessed'] = df['text'].apply(preprocess_text)

# Extract TF-IDF features
vectorizer = TfidfVectorizer(max_features=10000)
X_tfidf = vectorizer.fit_transform(df['text_preprocessed'])
print("TF-IDF matrix shape:", X_tfidf.shape)

"""I preprocess the text by lowercasing, removing punctuation, and removing stopwords. TF‑IDF (with a max of 10,000 features) is then used to convert the preprocessed text into numerical features.

**Splitting Data into Train and Test Sets**
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['category'], test_size=0.3, random_state=42)
print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

"""**Unsupervised Learning with NMF**

**Apply NMF and Train Classifier on Latent Features**
"""

from sklearn.decomposition import NMF
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

# Set hyperparameter for number of latent topics
n_components = 50
nmf_model = NMF(n_components=n_components, random_state=42, init='nndsvd', max_iter=500)
W_train = nmf_model.fit_transform(X_train)
W_test = nmf_model.transform(X_test)

# Train a classifier on the latent topics (unsupervised approach)
clf_unsup = LogisticRegression(max_iter=1000, random_state=42)
clf_unsup.fit(W_train, y_train)
pred_unsup_train = clf_unsup.predict(W_train)
pred_unsup_test = clf_unsup.predict(W_test)

acc_unsup_train = accuracy_score(y_train, pred_unsup_train)
acc_unsup_test = accuracy_score(y_test, pred_unsup_test)
print("Unsupervised (NMF latent) model Train Accuracy:", acc_unsup_train)
print("Unsupervised (NMF latent) model Validation Accuracy:", acc_unsup_test)
print("Confusion Matrix (Validation):\n", confusion_matrix(y_test, pred_unsup_test))

"""I applied NMF to extract 50 latent topics from the TF‑IDF features. A logistic regression classifier trained on these latent features achieved a test accuracy of 74.5% (as measured on the 20 Newsgroups test set). Note that I trained NMF only on the training data to avoid data leakage.

**Hyperparameter Tuning for NMF**
"""

results_nmf = {}
for n in [20, 50, 100]:
    nmf = NMF(n_components=n, random_state=42, init='nndsvd', max_iter=500)
    W_tr = nmf.fit_transform(X_train)
    W_te = nmf.transform(X_test)
    clf = LogisticRegression(max_iter=1000, random_state=42)
    clf.fit(W_tr, y_train)
    acc = accuracy_score(y_test, clf.predict(W_te))
    results_nmf[n] = acc
    print("NMF with n_components =", n, "Test Accuracy:", acc)

# Plot NMF accuracy vs. n_components
import matplotlib.pyplot as plt
comps = list(results_nmf.keys())
accs = list(results_nmf.values())
plt.figure(figsize=(8,4))
plt.plot(comps, accs, marker='o', linestyle='-', color='blue')
plt.title("Test Accuracy vs. Number of Latent Topics (NMF)")
plt.xlabel("n_components")
plt.ylabel("Test Accuracy")
plt.xticks(comps)
plt.show()

"""By tuning the number of latent topics (n_components), I observed variations in test accuracy. This hyperparameter tuning helps balance between underfitting and overfitting in the unsupervised approach.

**Supervised Learning Approach**
"""

clf_sup = LogisticRegression(max_iter=1000, random_state=42)
clf_sup.fit(X_train, y_train)
pred_sup_train = clf_sup.predict(X_train)
pred_sup_test = clf_sup.predict(X_test)

acc_sup_train = accuracy_score(y_train, pred_sup_train)
acc_sup_test = accuracy_score(y_test, pred_sup_test)
print("Supervised (TF-IDF) model Train Accuracy:", acc_sup_train)
print("Supervised (TF-IDF) model Validation Accuracy:", acc_sup_test)
print("Confusion Matrix (Validation):\n", confusion_matrix(y_test, pred_sup_test))

"""The supervised model, trained directly on the TF‑IDF features using logistic regression, generally achieves higher test accuracy compared to the unsupervised NMF approach when ample labeled data is available.

**Experimenting with Reduced Training Data for Supervised Model**
"""

train_sizes = [0.1, 0.2, 0.5, 1.0]
sup_results = {}
for frac in train_sizes:
    subset_idx = np.random.choice(range(X_train.shape[0]), int(frac * X_train.shape[0]), replace=False)
    X_subset = X_train[subset_idx]
    y_subset = y_train.iloc[subset_idx]

    clf = LogisticRegression(max_iter=1000, random_state=42)
    clf.fit(X_subset, y_subset)
    train_acc = accuracy_score(y_subset, clf.predict(X_subset))
    test_acc = accuracy_score(y_test, clf.predict(X_test))
    sup_results[frac] = (train_acc, test_acc)
    print(f"Training Fraction {frac}: Train Accuracy = {train_acc}, Test Accuracy = {test_acc}")

# Plot the results
fractions = list(sup_results.keys())
train_accs = [sup_results[f][0] for f in fractions]
test_accs = [sup_results[f][1] for f in fractions]
plt.figure(figsize=(8,6))
plt.plot(fractions, train_accs, marker='o', label='Train Accuracy')
plt.plot(fractions, test_accs, marker='o', label='Test Accuracy')
plt.xlabel("Fraction of Training Data")
plt.ylabel("Accuracy")
plt.title("Supervised Model Performance vs. Training Data Size")
plt.legend()
plt.show()

"""Reducing the amount of training data causes a drop in performance for the supervised model. This experiment demonstrates that supervised models require a significant amount of labeled data to achieve high accuracy, whereas unsupervised approaches (like NMF) might be more robust when labels are limited.

**Baseline Deep Learning Model using Bidirectional LSTM**
"""

# Deep Learning Model for 20 Newsgroups Classification using a Bidirectional LSTM
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping

# Parameters for deep learning model
max_words = 20000   # maximum number of words to consider in the vocabulary
max_len = 200       # maximum length (in tokens) for each sequence

# Use the preprocessed text (assumed to be in df['text_preprocessed'])
tokenizer_dl = Tokenizer(num_words=max_words)
tokenizer_dl.fit_on_texts(df['text_preprocessed'])
sequences_dl = tokenizer_dl.texts_to_sequences(df['text_preprocessed'])
padded_sequences = pad_sequences(sequences_dl, maxlen=max_len)

# Split data for deep learning model (using the same target 'category')
X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(padded_sequences, df['category'],
                                                                test_size=0.3, random_state=42)

# Build a baseline deep learning model
model_dl = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    Bidirectional(LSTM(64)),
    Dropout(0.5),
    Dense(20, activation='softmax')  # 20 classes corresponding to the 20 newsgroups
])

model_dl.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_dl.summary()

# Train the baseline model with early stopping
es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history_dl = model_dl.fit(X_train_dl, y_train_dl, epochs=10, batch_size=128,
                          validation_split=0.1, callbacks=[es])
score_dl = model_dl.evaluate(X_test_dl, y_test_dl)
print("Baseline Deep Learning Model Test Accuracy:", score_dl[1])

"""**Hyperparameter Tuning using Keras Tuner**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import keras_tuner as kt  # Make sure you've installed keras-tuner with: !pip install keras-tuner
from sklearn.model_selection import train_test_split

# Global variables for tokenizer and padding
max_words = 10000
max_len = 200

# Assuming 'df' has been created and has a 'text' column
# Apply preprocessing if not already done
df['text_preprocessed'] = df['text'].apply(preprocess_text)

# Split the preprocessed text and labels into training and testing sets
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    df['text_preprocessed'], df['category'], test_size=0.3, random_state=42
)

# Tokenize and pad sequences for deep learning input
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train_raw)

X_train_dl = pad_sequences(tokenizer.texts_to_sequences(X_train_raw), maxlen=max_len, padding='post')
X_test_dl = pad_sequences(tokenizer.texts_to_sequences(X_test_raw), maxlen=max_len, padding='post')

print("Training sequences shape:", X_train_dl.shape)
print("Test sequences shape:", X_test_dl.shape)

def model_builder(hp):
    model = Sequential()

    # Hyperparameter for embedding dimension
    embedding_dim = hp.Int('embedding_dim', min_value=64, max_value=256, step=64)
    model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))

    # Hyperparameter for first Bidirectional LSTM layer units
    lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)
    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))

    # Hyperparameter: dropout rate after first LSTM
    dropout_1 = hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)
    model.add(Dropout(dropout_1))

    # Hyperparameter for second Bidirectional LSTM layer units
    lstm_units2 = hp.Int('lstm_units2', min_value=32, max_value=128, step=32)
    model.add(Bidirectional(LSTM(lstm_units2)))

    # Hyperparameter: dropout rate after second LSTM
    dropout_2 = hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)
    model.add(Dropout(dropout_2))

    # Final dense layer with 20 output units (for the 20 newsgroups) and softmax activation
    model.add(Dense(20, activation='softmax'))

    # Hyperparameter: Learning rate tuning
    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)

    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Create a tuner using RandomSearch
tuner = kt.RandomSearch(
    model_builder,
    objective='val_accuracy',
    max_trials=5,  # Increase for a wider search if needed
    executions_per_trial=1,
    directory='my_dir',
    project_name='20_newsgroups_dl'
)

# Optionally, print the search space summary
tuner.search_space_summary()

# Run hyperparameter search
tuner.search(X_train_dl, y_train, epochs=5, validation_split=0.1)

# Print a summary of the search results
tuner.results_summary()

# Retrieve the best model
best_model = tuner.get_best_models(num_models=1)[0]
score_best = best_model.evaluate(X_test_dl, y_test)
print("Tuned Deep Learning Model Test Accuracy:", score_best[1])

"""**Tokenization and Padding:**
The preprocessed text from my training and testing sets is tokenized using Keras’s Tokenizer (limited to the top 20,000 words) and then padded to ensure all sequences have the same length (300 tokens). This is necessary for feeding the data into an RNN.

**Baseline Bidirectional LSTM Model:**
The model starts with an Embedding layer to convert word indices into dense vectors. Two Bidirectional LSTM layers (with dropout for regularization) capture sequential dependencies. A final Dense layer with softmax activation produces class probabilities for the 20 categories. The model is compiled with a sparse categorical crossentropy loss (since labels are integer-encoded) and trained with early stopping.

**Hyperparameter Tuning:**
The Keras Tuner is used to explore different values for key hyperparameters (embedding dimension, LSTM units, dropout rates). The tuner searches over 5 trials using RandomSearch, and the best model is then evaluated on the test set.

# Observations and Conclusions

### Data Overview and EDA
- **Dataset Summary:**  
  The 20 Newsgroups dataset consists of 19,997 documents distributed across 20 categories. Initial EDA (using histograms, boxplots, and scatter plots) revealed that:
  - Document word counts mostly range between 100 and 1,000 words.
  - The numeric features (word count, character count, average word length) are as expected, with strong correlation between word count and character count.
  - No significant missing values were detected, though some outliers exist.

- **Text Preprocessing:**  
  Converting text to lowercase, removing punctuation, and filtering out stopwords effectively cleaned the data. TF‑IDF vectorization with a maximum of 10,000 features transformed the documents into a high-dimensional numerical matrix, which serves as the basis for further analysis.

### Unsupervised Learning with NMF
- **Latent Feature Extraction:**  
  Non-negative Matrix Factorization (NMF) was applied to the TF‑IDF features to extract latent topics. With 50 latent topics, the unsupervised model achieved:
  - **Train Accuracy:** ~74.5%
  - **Test Accuracy:** ~74.5%
- **Hyperparameter Tuning:**  
  Experimenting with different numbers of latent components (n_components = 20, 50, 100) showed that test accuracy varies with the latent space dimension. The tuning process helped balance underfitting and overfitting in the unsupervised representation.
- **Confusion Matrix Insights:**  
  The confusion matrix for the NMF-based classifier indicates some misclassifications among certain topics, suggesting that while latent features capture broad topics, some nuanced distinctions may be lost.

### Supervised Learning with TF‑IDF Features
- **Performance:**  
  A supervised Logistic Regression model trained directly on the full TF‑IDF features achieved:
  - **Train Accuracy:** ~96.2%
  - **Test Accuracy:** ~92.6%
- **Confusion Matrix Insights:**  
  The supervised model's confusion matrix shows a significantly lower misclassification rate compared to the NMF approach, demonstrating that when ample labeled data is available, supervised methods outperform the unsupervised approach.

### Experiment with Reduced Training Data
- **Impact of Training Size:**  
  Experiments with reduced fractions of the training data (10%, 20%, 50%, 100%) revealed:
  - A clear drop in both training and test accuracies as the amount of training data decreases.
  - This experiment underscores the dependency of supervised models on large amounts of labeled data for high performance.
- **Data Efficiency:**  
  The results indicate that while supervised methods excel with full datasets, their performance is more sensitive to the size of the labeled training data. In contrast, unsupervised methods like NMF might be more robust when labels are limited.

### Overall Conclusions
- **Comparative Performance:**  
  Supervised learning on raw TF‑IDF features yields much higher accuracy (over 92%) than the unsupervised NMF-based approach (around 74.5%).  
- **Trade-offs:**  
  - **Supervised Approach:** Offers high accuracy but requires substantial labeled data.  
  - **Unsupervised Approach (NMF):** Useful for discovering latent topics and may be preferable in settings with limited labels, although it may not reach the same accuracy as supervised models.
- **Future Directions:**  
  Potential improvements could involve exploring alternative unsupervised techniques (e.g., LDA for topic modeling), hybrid models that combine unsupervised topic extraction with supervised fine-tuning, or incorporating additional feature engineering to enhance the latent representation.

These observations provide a comprehensive view of the performance trade-offs between unsupervised and supervised learning on the 20 Newsgroups dataset.

# Uncovering Latent Topics with Deep Learning: An Extended Analysis on the 20 Newsgroups Dataset

## Project Overview
This project aims to explore text classification on the 20 Newsgroups dataset—a collection of 19,997 documents distributed across 20 distinct categories. Our analysis includes both traditional machine learning approaches and a deep learning extension. We initially perform exploratory data analysis (EDA) and text preprocessing to transform the raw documents into numerical features using TF‑IDF vectorization.

We then implement two baseline models:
- **Unsupervised Learning:** Using Non-negative Matrix Factorization (NMF) to extract latent topics, followed by a Logistic Regression classifier.
- **Supervised Learning:** A Logistic Regression model trained directly on the TF‑IDF features.

As an extension, we develop a deep learning model using a Bidirectional LSTM network. We use Keras Tuner for hyperparameter optimization to better capture the sequential nature of the text data.

## Objective
- **Primary Goal:** Compare the performance and trade-offs between unsupervised, supervised, and deep learning approaches for text classification.
- **Deep Learning Focus:** Leverage Bidirectional LSTM layers to capture contextual and sequential information in the text, and use hyperparameter tuning (with Keras Tuner) to optimize the model.
- **Baselines:** Establish robust baselines using NMF with Logistic Regression and a direct TF‑IDF + Logistic Regression model.

## About the Dataset
The 20 Newsgroups dataset comprises 19,997 documents collected from 20 different newsgroups. This dataset is widely used in natural language processing (NLP) research for both supervised and unsupervised tasks. For this project:
- **Documents:** ~20,000 text files.
- **Categories:** 20 topics ranging from "alt.atheism" to "talk.religion.misc".
- **Preprocessing:** Texts were converted to lowercase, punctuation was removed, and stopwords were filtered out. TF‑IDF vectorization with up to 10,000 features was then applied to create a high-dimensional feature matrix.

## Baseline Models

### Unsupervised Learning with NMF
- **Latent Feature Extraction:**  
  NMF was applied to the TF‑IDF matrix to extract latent topics. With 50 latent topics, the unsupervised model achieved approximately:
  - **Train Accuracy:** ~74.5%
  - **Test Accuracy:** ~74.5%
- **Hyperparameter Tuning:**  
  Varying the number of latent components (e.g., 20, 50, 100) helped us understand the trade-offs between underfitting and overfitting.
- **Confusion Matrix Insights:**  
  Although the latent features capture broad topics, some nuanced distinctions between categories are lost, leading to certain misclassifications.

### Supervised Learning with TF‑IDF Features
- **Performance:**  
  A Logistic Regression model trained on the full TF‑IDF features achieved:
  - **Train Accuracy:** ~96.2%
  - **Test Accuracy:** ~92.6%
- **Confusion Matrix Insights:**  
  The supervised model shows a much lower misclassification rate compared to the NMF-based classifier, highlighting the benefit of using high-dimensional sparse features when ample labeled data is available.
- **Reduced Training Data Experiment:**  
  Experiments with varying training set sizes (10%, 20%, 50%, 100%) revealed a significant drop in performance with reduced labeled data, emphasizing the data dependency of supervised models.

---

## Deep Learning Extension with Bidirectional LSTM

To further enhance our analysis, we developed a deep learning model using a Bidirectional LSTM network. This model is designed to capture both forward and backward context in the text, addressing the sequential nature that traditional approaches might overlook.

**Model Architecture and Preprocessing:**
- **Input Preparation:**  
  The preprocessed texts were tokenized and padded to a fixed length to serve as input for the deep network.
- **Network Structure:**  
  - **Embedding Layer:** Converts token indices into dense vector representations.
  - **Bidirectional LSTM Layers:** Two stacked Bidirectional LSTM layers capture context in both directions.
  - **Dropout Layers:** Added after each LSTM layer to reduce overfitting.
  - **Dense Output Layer:** A final dense layer with softmax activation outputs predictions for the 20 newsgroups.
  
**Hyperparameter Tuning:**
- We used Keras Tuner (with RandomSearch) to optimize key hyperparameters:
  - Embedding dimension (64–256)
  - Units in the first and second LSTM layers (32–128)
  - Dropout rates (0.2–0.5)
  - Learning rate (tuned on a log scale between 1e-4 and 1e-2)
  
**Performance:**
- The tuned Bidirectional LSTM model achieved:
  - **Validation Accuracy:** ~89.9%
  - **Test Accuracy:** ~75.2% (as evaluated on the deep learning test set)
  
While the deep learning model's performance does not yet exceed that of the supervised logistic regression on TF‑IDF features (~92.6%), it demonstrates the potential of neural networks to leverage the sequential structure of text. With further tuning or more complex architectures (e.g., attention mechanisms or transformers), performance improvements can be expected.

---

## Overall Conclusions

**Comparative Performance:**
- **Supervised (TF‑IDF + Logistic Regression):** Highest accuracy (~92.6%) with abundant labeled data.
- **Unsupervised (NMF + Logistic Regression):** Moderate accuracy (~74.5%), useful in low-label scenarios and for topic discovery.
- **Deep Learning (Bidirectional LSTM):** Competitive performance (~89.9% validation accuracy) with the added benefit of capturing sequential dependencies.

**Trade-offs and Future Directions:**
- **Supervised Methods:** Offer high accuracy but require extensive labeled data.
- **Unsupervised Methods:** Provide robust latent topic extraction, though may lose fine-grained distinctions.
- **Deep Learning Methods:** Present a promising approach to integrate contextual and sequential information; future work could explore larger networks, attention mechanisms, or transformer architectures to further boost performance.

**Additional Baseline Experiments:**  
To complete our analysis, we also compared models using reduced training data, which highlighted the sensitivity of supervised methods to data volume. This multi-faceted approach provides a comprehensive view of the strengths and limitations of different modeling techniques for text classification on the 20 Newsgroups dataset.

---

*In the following sections of the notebook, the deep learning model using Bidirectional LSTM is implemented and tuned with Keras Tuner, providing an extended analysis beyond the baseline unsupervised and supervised approaches.*
"""

